{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ§  SV-NSDE: Semantic Volatility-Modulated Neural SDE\n",
    "\n",
    "**Modeling Crisis Dynamics: Distinguishing Panic from Trends in COVID-19 Information Diffusion**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/YOUR_USERNAME/YOUR_REPO/blob/main/notebooks/SV_NSDE_Colab.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‹ Notebook å†…å®¹\n",
    "\n",
    "1. **ç¯å¢ƒè®¾ç½®** - å®‰è£…ä¾èµ–\n",
    "2. **æ•°æ®å‡†å¤‡** - ç”Ÿæˆ/åŠ è½½æ•°æ®\n",
    "3. **æ¨¡å‹æ¶æ„** - SV-NSDEæ ¸å¿ƒç»„ä»¶\n",
    "4. **è®­ç»ƒæ¨¡å‹** - å®Œæ•´è®­ç»ƒæµç¨‹\n",
    "5. **è¯„ä¼°å¯¹æ¯”** - ä¸åŸºçº¿æ¨¡å‹å¯¹æ¯”\n",
    "6. **æ³¢åŠ¨ç‡åˆ†æ** - è¯†åˆ«ææ…Œäº‹ä»¶\n",
    "7. **å¯è§†åŒ–** - SDEè½¨è¿¹ä¸å¼ºåº¦\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# mount google drive to /content/drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Copy of 2019-12.csv'  'Copy of 2020-05.csv'  'Copy of 2020-10.csv'\n",
      "'Copy of 2020-01.csv'  'Copy of 2020-06.csv'  'Copy of 2020-11.csv'\n",
      "'Copy of 2020-02.csv'  'Copy of 2020-07.csv'  'Copy of 2020-12.csv'\n",
      "'Copy of 2020-03.csv'  'Copy of 2020-08.csv'  'Copy of user.csv'\n",
      "'Copy of 2020-04.csv'  'Copy of 2020-09.csv'\n"
     ]
    }
   ],
   "source": [
    "! ls drive/My\\ Drive/WeiboCOV "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ ç¯å¢ƒè®¾ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ£€æŸ¥æ˜¯å¦åœ¨ Colab ç¯å¢ƒ\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"ğŸš€ Running in Google Colab\")\n",
    "    # å…‹éš†ä»“åº“\n",
    "    !git clone https://github.com/trumpool/Research.git sv_nsde_repo\n",
    "    %cd sv_nsde_repo\n",
    "    \n",
    "    # å®‰è£…ä¾èµ–\n",
    "    !pip install -q torch numpy transformers tqdm matplotlib seaborn\n",
    "    !pip install -e .\n",
    "else:\n",
    "    print(\"ğŸ’» Running locally\")\n",
    "    # æœ¬åœ°è¿è¡Œæ—¶ï¼Œç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•\n",
    "    import os\n",
    "    if os.path.exists('../src'):\n",
    "        sys.path.insert(0, '../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ£€æŸ¥ GPU\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"ğŸ–¥ï¸ Device: {device}\")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥æ‰€éœ€æ¨¡å—\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# è®¾ç½®ç»˜å›¾é£æ ¼\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ“ Libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ æ•°æ®å‡†å¤‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥ SV-NSDE æ¨¡å—\n",
    "from sv_nsde import (\n",
    "    SVNSDELite,\n",
    "    generate_synthetic_weibo_data,\n",
    "    WeiboCOVLoader,\n",
    "    CascadeDataset,\n",
    "    get_baseline,\n",
    "    Evaluator,\n",
    ")\n",
    "\n",
    "print(\"âœ“ SV-NSDE modules imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆåˆæˆæ•°æ® (æ— éœ€ä¸‹è½½)\n",
    "import os\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "print(\"ğŸ“Š Generating synthetic Weibo-COV data...\")\n",
    "df = generate_synthetic_weibo_data(\n",
    "    n_cascades=300,  # çº§è”æ•°é‡\n",
    "    output_path=\"data/synthetic_weibo.csv\",\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Generated {len(df):,} events\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½å¹¶æ„å»ºçº§è”\n",
    "loader = WeiboCOVLoader(\"data/synthetic_weibo.csv\")\n",
    "loader.load()\n",
    "cascades = loader.build_cascades(min_size=5, max_size=100)\n",
    "\n",
    "# è·å–ç»Ÿè®¡ä¿¡æ¯\n",
    "stats = loader.get_statistics()\n",
    "print(f\"\\nğŸ“ˆ Dataset Statistics:\")\n",
    "print(f\"   Cascades: {stats['num_cascades']}\")\n",
    "print(f\"   Total events: {stats['total_events']}\")\n",
    "print(f\"   Avg size: {stats['size_mean']:.1f} Â± {stats['size_std']:.1f}\")\n",
    "print(f\"   Max size: {stats['size_max']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŒ‰æ—¶é—´åˆ†å‰²æ•°æ®\n",
    "train_cascades, val_cascades, test_cascades = loader.split_by_time(\n",
    "    train_end=\"2020-02-29\",  # çˆ†å‘æœŸ\n",
    "    val_end=\"2020-03-31\"     # å¹³å°æœŸ\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ“‚ Data Split:\")\n",
    "print(f\"   Train: {len(train_cascades)} cascades (outbreak period)\")\n",
    "print(f\"   Val:   {len(val_cascades)} cascades (plateau period)\")\n",
    "print(f\"   Test:  {len(test_cascades)} cascades (decline period)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–çº§è”å¤§å°åˆ†å¸ƒ\n",
    "sizes = [c.size for c in cascades]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# ç›´æ–¹å›¾\n",
    "axes[0].hist(sizes, bins=30, edgecolor='white', alpha=0.7)\n",
    "axes[0].set_xlabel('Cascade Size')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Distribution of Cascade Sizes')\n",
    "axes[0].axvline(np.mean(sizes), color='red', linestyle='--', label=f'Mean: {np.mean(sizes):.1f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Log-log (æ£€æŸ¥å¹‚å¾‹åˆ†å¸ƒ)\n",
    "axes[1].hist(sizes, bins=np.logspace(0, np.log10(max(sizes)), 30), edgecolor='white', alpha=0.7)\n",
    "axes[1].set_xscale('log')\n",
    "axes[1].set_yscale('log')\n",
    "axes[1].set_xlabel('Cascade Size (log)')\n",
    "axes[1].set_ylabel('Frequency (log)')\n",
    "axes[1].set_title('Power-Law Check (Log-Log)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ æ¨¡å‹æ¶æ„\n",
    "\n",
    "### SV-NSDE æ ¸å¿ƒç»„ä»¶\n",
    "\n",
    "**è€¦åˆSDEç³»ç»Ÿ (Equation 3):**\n",
    "\n",
    "$$dz(t) = \\mu_\\theta(z,t)dt + \\sqrt{v(t)} \\odot dW_z(t) + J_\\phi(z(t^-), x_i)dN(t)$$\n",
    "\n",
    "$$dv(t) = \\kappa(\\theta - v(t))dt + \\xi\\sqrt{v(t)} \\odot dW_v(t)$$\n",
    "\n",
    "**åŒé€šé“å¼ºåº¦å‡½æ•° (Equation 4):**\n",
    "\n",
    "$$\\lambda(t) = \\text{Softplus}\\left(\\underbrace{w_{tr}^\\top m(z(t))}_{\\text{Trend}} + \\underbrace{w_{vol}^\\top g(v(t))}_{\\text{Volatility}} + \\mu_{base}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é…ç½®å‚æ•°\n",
    "D_LATENT = 32   # æ½œåœ¨ç©ºé—´ç»´åº¦\n",
    "D_HIDDEN = 64   # éšè—å±‚ç»´åº¦\n",
    "\n",
    "print(f\"âš™ï¸ Model Configuration:\")\n",
    "print(f\"   d_latent: {D_LATENT}\")\n",
    "print(f\"   d_hidden: {D_HIDDEN}\")\n",
    "print(f\"   device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»º SV-NSDE æ¨¡å‹\n",
    "model = SVNSDELite(\n",
    "    d_input=D_LATENT,\n",
    "    d_latent=D_LATENT,\n",
    "    d_hidden=D_HIDDEN,\n",
    ").to(device)\n",
    "\n",
    "# æ‰“å°æ¨¡å‹ç»“æ„\n",
    "print(\"\\nğŸ§  SV-NSDE Architecture:\")\n",
    "print(\"=\" * 50)\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {n_params:,}\")\n",
    "print(\"\\nComponents:\")\n",
    "for name, module in model.named_children():\n",
    "    params = sum(p.numel() for p in module.parameters())\n",
    "    print(f\"  {name}: {params:,} params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŸ¥çœ‹ SDE ç»„ä»¶\n",
    "print(\"\\nğŸ“ Neural Heston SDE:\")\n",
    "print(\"  Drift: Î¼_Î¸(z, t) - neural network\")\n",
    "print(\"  Diffusion: âˆšv(t) - stochastic volatility\")\n",
    "print(\"  Jump: J_Ï†(z, x) - event-driven updates\")\n",
    "print(f\"\\n  CIR parameters:\")\n",
    "print(f\"    Îº (mean reversion): {model.sde.sde_func.kappa.item():.4f}\")\n",
    "print(f\"    Î¸ (long-term var):  {model.sde.sde_func.theta.item():.4f}\")\n",
    "print(f\"    Î¾ (vol of vol):     {model.sde.sde_func.xi.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ– SDE è·¯å¾„é‡‡æ · (æ— äº‹ä»¶)\n",
    "from sv_nsde.sde import NeuralHestonSDE\n",
    "\n",
    "# åˆ›å»º2ç»´SDEç”¨äºå¯è§†åŒ–\n",
    "sde_2d = NeuralHestonSDE(d_latent=2).to(device)\n",
    "\n",
    "# é‡‡æ ·å¤šæ¡è·¯å¾„\n",
    "with torch.no_grad():\n",
    "    z_paths, v_paths, times = sde_2d.sample_paths(\n",
    "        n_paths=10,\n",
    "        T=1.0,\n",
    "        dt=0.01,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "z_paths = z_paths.cpu().numpy()\n",
    "v_paths = v_paths.cpu().numpy()\n",
    "\n",
    "# ç»˜å›¾\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# z(t) è½¨è¿¹\n",
    "for i in range(z_paths.shape[1]):\n",
    "    axes[0].plot(times, z_paths[:, i, 0], alpha=0.7)\n",
    "axes[0].set_xlabel('Time')\n",
    "axes[0].set_ylabel('z(t) dim 0')\n",
    "axes[0].set_title('Semantic Trajectory z(t)')\n",
    "\n",
    "# v(t) æ³¢åŠ¨ç‡\n",
    "for i in range(v_paths.shape[1]):\n",
    "    axes[1].plot(times, v_paths[:, i, 0], alpha=0.7)\n",
    "axes[1].set_xlabel('Time')\n",
    "axes[1].set_ylabel('v(t) dim 0')\n",
    "axes[1].set_title('Volatility Process v(t)')\n",
    "\n",
    "# 2D ç›¸ç©ºé—´\n",
    "for i in range(min(5, z_paths.shape[1])):\n",
    "    axes[2].plot(z_paths[:, i, 0], z_paths[:, i, 1], alpha=0.7)\n",
    "    axes[2].scatter(z_paths[0, i, 0], z_paths[0, i, 1], s=50, marker='o')  # èµ·ç‚¹\n",
    "    axes[2].scatter(z_paths[-1, i, 0], z_paths[-1, i, 1], s=50, marker='x')  # ç»ˆç‚¹\n",
    "axes[2].set_xlabel('z dim 0')\n",
    "axes[2].set_ylabel('z dim 1')\n",
    "axes[2].set_title('2D Phase Space')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ è®­ç»ƒæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‡†å¤‡è®­ç»ƒæ•°æ® (ä½¿ç”¨éšæœºåµŒå…¥ä»£æ›¿BERT)\n",
    "print(\"ğŸ“¦ Preparing training data...\")\n",
    "\n",
    "train_embeddings = {}\n",
    "val_embeddings = {}\n",
    "\n",
    "for c in train_cascades:\n",
    "    train_embeddings[c.cascade_id] = torch.randn(c.size, D_LATENT)\n",
    "\n",
    "for c in val_cascades:\n",
    "    val_embeddings[c.cascade_id] = torch.randn(c.size, D_LATENT)\n",
    "\n",
    "print(f\"âœ“ Train embeddings: {len(train_embeddings)}\")\n",
    "print(f\"âœ“ Val embeddings: {len(val_embeddings)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®­ç»ƒé…ç½®\n",
    "LEARNING_RATE = 1e-3\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 20  # Colabä¸Šå¯ä»¥å¢åŠ åˆ°50-100\n",
    "\n",
    "print(f\"\\nğŸ¯ Training Configuration:\")\n",
    "print(f\"   Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   Epochs: {NUM_EPOCHS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç®€å•è®­ç»ƒå¾ªç¯\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# é‡æ–°åˆ›å»ºæ¨¡å‹\n",
    "model = SVNSDELite(\n",
    "    d_input=D_LATENT,\n",
    "    d_latent=D_LATENT,\n",
    "    d_hidden=D_HIDDEN,\n",
    ").to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS, eta_min=LEARNING_RATE * 0.01)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "print(\"\\nğŸš€ Starting training...\\n\")\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    n_batches = 0\n",
    "    \n",
    "    # è®­ç»ƒ\n",
    "    for cascade in train_cascades:\n",
    "        if cascade.size < 3:\n",
    "            continue\n",
    "            \n",
    "        event_times = cascade.event_times.to(device)\n",
    "        event_marks = train_embeddings[cascade.cascade_id].to(device)\n",
    "        \n",
    "        # å½’ä¸€åŒ–æ—¶é—´\n",
    "        T = event_times.max().item()\n",
    "        if T > 0:\n",
    "            event_times = event_times / T\n",
    "        \n",
    "        # å‰å‘ä¼ æ’­\n",
    "        loss_dict = model.compute_loss(event_times, event_marks, T=1.0)\n",
    "        loss = loss_dict['loss']\n",
    "        \n",
    "        # åå‘ä¼ æ’­\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        n_batches += 1\n",
    "    \n",
    "    avg_train_loss = epoch_loss / max(n_batches, 1)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    # éªŒè¯\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    n_val = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for cascade in val_cascades:\n",
    "            if cascade.size < 3:\n",
    "                continue\n",
    "            \n",
    "            event_times = cascade.event_times.to(device)\n",
    "            event_marks = val_embeddings[cascade.cascade_id].to(device)\n",
    "            \n",
    "            T = event_times.max().item()\n",
    "            if T > 0:\n",
    "                event_times = event_times / T\n",
    "            \n",
    "            loss_dict = model.compute_loss(event_times, event_marks, T=1.0)\n",
    "            val_loss += loss_dict['loss'].item()\n",
    "            n_val += 1\n",
    "    \n",
    "    avg_val_loss = val_loss / max(n_val, 1)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    # æ‰“å°è¿›åº¦\n",
    "    if epoch % 5 == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch:3d}/{NUM_EPOCHS}: train_loss={avg_train_loss:.2f}, val_loss={avg_val_loss:.2f}\")\n",
    "\n",
    "print(\"\\nâœ“ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–è®­ç»ƒæ›²çº¿\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ax.plot(range(1, len(train_losses)+1), train_losses, label='Train Loss', marker='o', markersize=3)\n",
    "ax.plot(range(1, len(val_losses)+1), val_losses, label='Val Loss', marker='s', markersize=3)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Training Curves')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ è¯„ä¼°ä¸åŸºçº¿å¯¹æ¯”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºåŸºçº¿æ¨¡å‹\n",
    "print(\"ğŸ—ï¸ Initializing baseline models...\")\n",
    "\n",
    "baselines = {\n",
    "    \"SV-NSDE (ours)\": model,\n",
    "    \"RMTPP\": get_baseline(\"rmtpp\", d_input=D_LATENT, d_latent=D_LATENT, d_hidden=D_HIDDEN),\n",
    "    \"Neural Hawkes\": get_baseline(\"neural_hawkes\", d_input=D_LATENT, d_latent=D_LATENT, d_hidden=D_HIDDEN),\n",
    "    \"Latent ODE\": get_baseline(\"latent_ode\", d_input=D_LATENT, d_latent=D_LATENT, d_hidden=D_HIDDEN),\n",
    "    \"Neural Jump SDE\": get_baseline(\"neural_jump_sde\", d_input=D_LATENT, d_latent=D_LATENT, d_hidden=D_HIDDEN),\n",
    "}\n",
    "\n",
    "for name, m in baselines.items():\n",
    "    m.to(device)\n",
    "    params = sum(p.numel() for p in m.parameters())\n",
    "    print(f\"  {name}: {params:,} params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‡†å¤‡æµ‹è¯•æ•°æ®\n",
    "test_embeddings = {}\n",
    "for c in test_cascades:\n",
    "    test_embeddings[c.cascade_id] = torch.randn(c.size, D_LATENT)\n",
    "\n",
    "print(f\"ğŸ“Š Test data: {len(test_cascades)} cascades\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¿è¡Œè¯„ä¼°\n",
    "print(\"\\nğŸ“ Running evaluation...\\n\")\n",
    "\n",
    "evaluator = Evaluator(device=device)\n",
    "\n",
    "results = {}\n",
    "for name, m in baselines.items():\n",
    "    print(f\"Evaluating {name}...\")\n",
    "    try:\n",
    "        metrics = evaluator.evaluate_model(\n",
    "            m, \n",
    "            test_cascades[:30],  # ä½¿ç”¨å­é›†åŠ å¿«é€Ÿåº¦\n",
    "            test_embeddings,\n",
    "            verbose=False\n",
    "        )\n",
    "        results[name] = metrics\n",
    "        print(f\"  LL={metrics.log_likelihood:.2f}, Cosine={metrics.semantic_cosine:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–å¯¹æ¯”ç»“æœ\n",
    "import pandas as pd\n",
    "\n",
    "# åˆ›å»ºç»“æœè¡¨æ ¼\n",
    "rows = []\n",
    "for name, metrics in results.items():\n",
    "    rows.append({\n",
    "        'Model': name,\n",
    "        'Log-Likelihood': metrics.log_likelihood,\n",
    "        'Semantic Cosine': metrics.semantic_cosine,\n",
    "        'Time RMSE': metrics.time_rmse,\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(rows)\n",
    "print(\"\\nğŸ“Š Comparison Results:\")\n",
    "print(df_results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŸ±çŠ¶å›¾å¯¹æ¯”\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "models = list(results.keys())\n",
    "colors = plt.cm.Set2(np.linspace(0, 1, len(models)))\n",
    "\n",
    "# Log-Likelihood\n",
    "lls = [results[m].log_likelihood for m in models]\n",
    "valid_lls = [(m, ll) for m, ll in zip(models, lls) if not np.isnan(ll)]\n",
    "if valid_lls:\n",
    "    m_names, ll_vals = zip(*valid_lls)\n",
    "    bars = axes[0].bar(m_names, ll_vals, color=colors[:len(m_names)])\n",
    "    axes[0].set_ylabel('Log-Likelihood â†‘')\n",
    "    axes[0].set_title('Model Fit (higher is better)')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Semantic Cosine\n",
    "cosines = [results[m].semantic_cosine for m in models]\n",
    "valid_cos = [(m, c) for m, c in zip(models, cosines) if not np.isnan(c)]\n",
    "if valid_cos:\n",
    "    m_names, cos_vals = zip(*valid_cos)\n",
    "    bars = axes[1].bar(m_names, cos_vals, color=colors[:len(m_names)])\n",
    "    axes[1].set_ylabel('Cosine Similarity â†‘')\n",
    "    axes[1].set_title('Semantic Prediction (higher is better)')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ æ³¢åŠ¨ç‡åˆ†æ (æ ¸å¿ƒåŠŸèƒ½)\n",
    "\n",
    "**å…³é”®é—®é¢˜**: èƒ½å¦åŒºåˆ†ææ…Œé©±åŠ¨çš„äº‹ä»¶ vs è¶‹åŠ¿é©±åŠ¨çš„äº‹ä»¶ï¼Ÿ\n",
    "\n",
    "$$\\lambda(t) = \\text{Softplus}\\left(\\underbrace{w_{tr}^\\top m(z(t))}_{\\text{Trend Channel}} + \\underbrace{w_{vol}^\\top g(v(t))}_{\\text{Volatility Channel}} + \\mu_{base}\\right)$$\n",
    "\n",
    "- **Trend-driven events**: ç”±è¯­ä¹‰çŠ¶æ€z(t)é©±åŠ¨ï¼ˆå¦‚ï¼šå‘å¸ƒç–«æƒ…æ•°æ®ï¼‰\n",
    "- **Panic-driven events**: ç”±æ³¢åŠ¨ç‡v(t)é©±åŠ¨ï¼ˆå¦‚ï¼šè°£è¨€å¼•å‘çš„ææ…Œï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ†æå•ä¸ªçº§è”\n",
    "from sv_nsde import VolatilityAnalyzer\n",
    "\n",
    "analyzer = VolatilityAnalyzer(model, device=device)\n",
    "\n",
    "# é€‰æ‹©ä¸€ä¸ªæµ‹è¯•çº§è”\n",
    "sample_cascade = test_cascades[0]\n",
    "sample_embeddings = test_embeddings[sample_cascade.cascade_id]\n",
    "\n",
    "print(f\"\\nğŸ” Analyzing cascade: {sample_cascade.cascade_id}\")\n",
    "print(f\"   Size: {sample_cascade.size} events\")\n",
    "print(f\"   Duration: {sample_cascade.duration:.2f} hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ³¢åŠ¨ç‡åˆ†è§£\n",
    "analysis = analyzer.analyze_cascade(sample_cascade, sample_embeddings)\n",
    "\n",
    "print(f\"\\nğŸ“Š Volatility Decomposition:\")\n",
    "print(f\"   Panic-driven events: {analysis['num_panic_events']}/{sample_cascade.size}\")\n",
    "print(f\"   Panic ratio: {analysis['panic_ratio']:.2%}\")\n",
    "print(f\"\\n   Volatility ratio (first 5 events): {analysis['volatility_ratio'][:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–åˆ†è§£ç»“æœ\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "times = analysis['event_times']\n",
    "trend = analysis['trend_contribution']\n",
    "vol = analysis['volatility_contribution']\n",
    "vol_ratio = analysis['volatility_ratio']\n",
    "\n",
    "# è¶‹åŠ¿ vs æ³¢åŠ¨ç‡è´¡çŒ®\n",
    "ax = axes[0, 0]\n",
    "ax.plot(times, trend, label='Trend Channel', marker='o', markersize=4)\n",
    "ax.plot(times, vol, label='Volatility Channel', marker='s', markersize=4)\n",
    "ax.set_xlabel('Time (hours)')\n",
    "ax.set_ylabel('Contribution')\n",
    "ax.set_title('Intensity Decomposition')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# æ³¢åŠ¨ç‡æ¯”ä¾‹\n",
    "ax = axes[0, 1]\n",
    "colors = ['red' if r > 0.5 else 'blue' for r in vol_ratio]\n",
    "ax.bar(range(len(vol_ratio)), vol_ratio, color=colors, alpha=0.7)\n",
    "ax.axhline(0.5, color='black', linestyle='--', label='Threshold (0.5)')\n",
    "ax.set_xlabel('Event Index')\n",
    "ax.set_ylabel('Volatility Ratio')\n",
    "ax.set_title('Panic vs Trend Classification')\n",
    "ax.legend()\n",
    "\n",
    "# ç´¯ç§¯äº‹ä»¶\n",
    "ax = axes[1, 0]\n",
    "cumulative = np.arange(1, len(times) + 1)\n",
    "ax.plot(times, cumulative, marker='o', markersize=4)\n",
    "ax.set_xlabel('Time (hours)')\n",
    "ax.set_ylabel('Cumulative Events')\n",
    "ax.set_title('Cascade Growth Pattern')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# äº‹ä»¶ç±»å‹é¥¼å›¾\n",
    "ax = axes[1, 1]\n",
    "n_panic = analysis['num_panic_events']\n",
    "n_trend = len(times) - n_panic\n",
    "ax.pie([n_trend, n_panic], labels=['Trend-driven', 'Panic-driven'], \n",
    "       colors=['#3498db', '#e74c3c'], autopct='%1.1f%%', startangle=90)\n",
    "ax.set_title('Event Type Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ†ææ‰€æœ‰æµ‹è¯•çº§è”\n",
    "print(\"\\nğŸ“Š Analyzing all test cascades...\")\n",
    "\n",
    "all_panic_ratios = []\n",
    "for cascade in tqdm(test_cascades[:50]):\n",
    "    if cascade.cascade_id not in test_embeddings:\n",
    "        continue\n",
    "    try:\n",
    "        analysis = analyzer.analyze_cascade(\n",
    "            cascade, \n",
    "            test_embeddings[cascade.cascade_id]\n",
    "        )\n",
    "        all_panic_ratios.append({\n",
    "            'cascade_id': cascade.cascade_id,\n",
    "            'size': cascade.size,\n",
    "            'panic_ratio': analysis['panic_ratio'],\n",
    "            'num_panic': analysis['num_panic_events'],\n",
    "        })\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "df_panic = pd.DataFrame(all_panic_ratios)\n",
    "print(f\"\\nğŸ“ˆ Panic Analysis Summary:\")\n",
    "print(f\"   Mean panic ratio: {df_panic['panic_ratio'].mean():.2%}\")\n",
    "print(f\"   Std panic ratio: {df_panic['panic_ratio'].std():.2%}\")\n",
    "print(f\"   High-panic cascades (>50%): {(df_panic['panic_ratio'] > 0.5).sum()}/{len(df_panic)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–ææ…Œæ¯”ä¾‹åˆ†å¸ƒ\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# ç›´æ–¹å›¾\n",
    "axes[0].hist(df_panic['panic_ratio'], bins=20, edgecolor='white', alpha=0.7)\n",
    "axes[0].axvline(0.5, color='red', linestyle='--', label='Threshold')\n",
    "axes[0].set_xlabel('Panic Ratio')\n",
    "axes[0].set_ylabel('Number of Cascades')\n",
    "axes[0].set_title('Distribution of Panic Ratios')\n",
    "axes[0].legend()\n",
    "\n",
    "# æ•£ç‚¹å›¾: çº§è”å¤§å° vs ææ…Œæ¯”ä¾‹\n",
    "axes[1].scatter(df_panic['size'], df_panic['panic_ratio'], alpha=0.6)\n",
    "axes[1].set_xlabel('Cascade Size')\n",
    "axes[1].set_ylabel('Panic Ratio')\n",
    "axes[1].set_title('Size vs Panic Ratio')\n",
    "axes[1].axhline(0.5, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7ï¸âƒ£ ä¿å­˜æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿å­˜æ¨¡å‹æƒé‡\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'config': {\n",
    "        'd_latent': D_LATENT,\n",
    "        'd_hidden': D_HIDDEN,\n",
    "    },\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses,\n",
    "}, 'checkpoints/sv_nsde_model.pt')\n",
    "\n",
    "print(\"âœ“ Model saved to checkpoints/sv_nsde_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½æ¨¡å‹ç¤ºä¾‹\n",
    "checkpoint = torch.load('checkpoints/sv_nsde_model.pt', map_location=device)\n",
    "\n",
    "loaded_model = SVNSDELite(\n",
    "    d_input=checkpoint['config']['d_latent'],\n",
    "    d_latent=checkpoint['config']['d_latent'],\n",
    "    d_hidden=checkpoint['config']['d_hidden'],\n",
    ").to(device)\n",
    "\n",
    "loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "loaded_model.eval()\n",
    "\n",
    "print(\"âœ“ Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š æ€»ç»“\n",
    "\n",
    "### æœ¬ Notebook å®Œæˆäº†:\n",
    "\n",
    "1. âœ… **ç¯å¢ƒè®¾ç½®** - å®‰è£…ä¾èµ–å¹¶æ£€æµ‹GPU\n",
    "2. âœ… **æ•°æ®å‡†å¤‡** - ç”Ÿæˆåˆæˆæ•°æ®å¹¶æ„å»ºçº§è”\n",
    "3. âœ… **æ¨¡å‹æ¶æ„** - ç†è§£SV-NSDEçš„æ ¸å¿ƒç»„ä»¶\n",
    "4. âœ… **è®­ç»ƒæ¨¡å‹** - å®Œæ•´çš„è®­ç»ƒå¾ªç¯\n",
    "5. âœ… **è¯„ä¼°å¯¹æ¯”** - ä¸4ä¸ªåŸºçº¿æ¨¡å‹æ¯”è¾ƒ\n",
    "6. âœ… **æ³¢åŠ¨ç‡åˆ†æ** - è¯†åˆ«ææ…Œé©±åŠ¨çš„äº‹ä»¶\n",
    "7. âœ… **ä¿å­˜æ¨¡å‹** - å¯¼å‡ºè®­ç»ƒå¥½çš„æ¨¡å‹\n",
    "\n",
    "### ä¸‹ä¸€æ­¥:\n",
    "\n",
    "- ğŸ”— ä¸‹è½½çœŸå® Weibo-COV æ•°æ®: https://github.com/nghuyong/weibo-cov\n",
    "- ğŸ”§ ä½¿ç”¨ RoBERTa ç¼–ç çœŸå®å¾®åšæ–‡æœ¬\n",
    "- ğŸ“ˆ å¢åŠ è®­ç»ƒ epochs è·å¾—æ›´å¥½æ€§èƒ½\n",
    "- ğŸ§ª è¿›è¡Œæ¶ˆèå®éªŒéªŒè¯æ³¢åŠ¨ç‡é€šé“çš„ä½œç”¨\n",
    "\n",
    "---\n",
    "\n",
    "**å‚è€ƒè®ºæ–‡**: Chen, Z. (2026). Modeling Crisis Dynamics with Volatility-Modulated Neural SDEs."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
